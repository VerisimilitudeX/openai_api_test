{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Setting Up\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Banks are always concerned with fraud to prevent from those intending to misuse the system to take advantage of it. In this section, we will build a classification model to predict whether or not an applicant is committing bank fraud. We will practice to use Decision Trees for the purpose.Along with that, we will also develop our own custom classifier class, a majority class classifier."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn import metrics\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "np.random.seed(416)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's create a helper function to create confusion matrices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "\n",
                "def visualize_confusion_matrix(test, pred, score):\n",
                "    cm = confusion_matrix(test, pred)\n",
                "    plt.figure(figsize=(9,9))\n",
                "    sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
                "    plt.ylabel('Actual label')\n",
                "    plt.xlabel('Predicted label')\n",
                "    all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
                "    plt.title(all_sample_title, size = 15)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Inspecting Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "bank_fraud = pd.read_csv('bank_fraud.csv')\n",
                "bank_fraud = bank_fraud.drop(columns = ['Unnamed: 0'])\n",
                "print(len(bank_fraud))\n",
                "bank_fraud.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Pre-processing"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data revamp \n",
                "The target column (label column) of the dataset that we are interested in is called `fraud_bool`. In this column 1 means fraudulent and 0 means  legitimate application.\n",
                "\n",
                "In order to make this more intuitive and consistent with the lectures, we reassign the target to be:\n",
                "\n",
                "* +1 as legitimate,\n",
                "* -1 as fraud.\n",
                "\n",
                "We put this in a new column called `not_fraud`. This column will be the target values you will use while training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "bank_fraud['not_fraud'] = bank_fraud['fraud_bool'].apply(lambda x : +1 if x==0 else -1)\n",
                "\n",
                "# Drop the old fraud_bool column\n",
                "bank_fraud = bank_fraud.drop(columns='fraud_bool')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, let's see the proportion of fraudulent and not fraudulent applications in our data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "only_legit = bank_fraud[bank_fraud['not_fraud'] == 1]\n",
                "only_fraud = bank_fraud[bank_fraud['not_fraud'] == -1]\n",
                "\n",
                "print(f'Number of legit applications: {len(only_legit)} ({len(only_legit) * 100.0 / len(bank_fraud):.2f}%)')\n",
                "print(f'Number of fraud applications: {len(only_fraud)} ({len(only_fraud) * 100.0 / len(bank_fraud):.2f}%)')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Selection\n",
                "We will be using the following features in the fraud dataset. These values are both numeric and categorical"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "features = [\n",
                "    'income', 'name_email_similarity', 'prev_address_months_count',\n",
                "       'current_address_months_count', 'customer_age', 'days_since_request',\n",
                "       'intended_balcon_amount', 'payment_type', 'zip_count_4w', 'velocity_6h',\n",
                "       'velocity_24h', 'velocity_4w', 'bank_branch_count_8w',\n",
                "       'date_of_birth_distinct_emails_4w', 'employment_status',\n",
                "       'credit_risk_score', 'email_is_free', 'housing_status',\n",
                "       'phone_home_valid', 'phone_mobile_valid', 'bank_months_count',\n",
                "       'has_other_cards', 'proposed_credit_limit', 'foreign_request', 'source',\n",
                "       'session_length_in_minutes', 'device_os', 'keep_alive_session',\n",
                "       'device_distinct_emails_8w', 'device_fraud_count', 'month' \n",
                "]\n",
                "\n",
                "target = 'not_fraud'   \n",
                "\n",
                "# Extract the feature columns and target column\n",
                "bank_fraud = bank_fraud[features + [target]]\n",
                "bank_fraud.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's take a look at all the features we have in colums again"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"features: \", bank_fraud.columns)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "With how `sklearn` implemented its `DecisionTreeClassifier`, it is only able to handle numerical features; the technical reason being it only allows binary splits and assumes (like other `sklearn` models), that the data is only numerical. This means we need to translate the categorical features into numeric ones. \n",
                "\n",
                "The easiest way to do this is a **one-hot encoding** of each categorical feature. We used this method in Section 2 for the same reasons. A one hot encoding of a feature creates new, derived, features that take on the value 0 or 1 (one new feature for each possible value of the original). This allows us to turn a categorical feature into a series of binary (0 or 1) numerical features.\n",
                "\n",
                "For example, if we had the dataset shown below where 'Gender' takes on values 'M' (Male), 'F' (Female), 'O' (Other) and 'HasHouse' takes on values 'Y' (Yes), and 'N' (No). This dataset would not work by default in scikit-learn since it has features with categorical values.\n",
                "\n",
                "| Gender | HasHouse | Age |\n",
                "|--------|----------|-----|\n",
                "| M      | N        | 19  |\n",
                "| F      | Y        | 23  |\n",
                "| O      | Y        | 24  |\n",
                "| F      | N        | 21  |\n",
                "\n",
                "To fix this, we could use a one-hot encoding to transform the categorical features into numeric ones. A one-hot encoding of the categorical features would be \n",
                "\n",
                "| Gender_M | Gender_F | Gender_O | HasHouse_N | HasHouse_Y | Age |\n",
                "|----------|----------|----------|------------|------------|-----|\n",
                "| 1        | 0        | 0        | 1          | 0          | 19  |\n",
                "| 0        | 1        | 0        | 0          | 1          | 23  |\n",
                "| 0        | 0        | 1        | 0          | 1          | 24  |\n",
                "| 0        | 1        | 0        | 1          | 0          | 21  |\n",
                "\n",
                "Note that the original 'Gender' column has been transformed into three related columns 'Gender_M', 'Gender_F', and 'Gender_O' which contains zeros or ones depending on the rows' original value. For each row, only one of these derived columns will contain a one (hence the name one-hot encoding).\n",
                "\n",
                "To do this in `pandas`, we use the `get_dummies()` method (another name for one-hot encoding, is dummy variable). We show the new features of the data after running the function in the next cell."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "bank_fraud = pd.get_dummies(bank_fraud)\n",
                "features = list(bank_fraud.columns)\n",
                "features.remove('not_fraud')\n",
                "features"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train Test Split\n",
                "\n",
                "Let's split our data in standard train-test split manner"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_val, test_data = train_test_split(bank_fraud, test_size = 0.1)\n",
                "train_data, validation_data = train_test_split(train_val, test_size=0.2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5. Decision Tree Classifiers"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we will code a decision tree classifier for the same data. See the documentation for [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
                "\n",
                "**Conceptual Question**: Is \"fraudulent bank applications\" the type of classification problem that decision trees are well-suited for? Why or why not?\n",
                "\n",
                "**Answer:** "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Complete the code block below"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the decision tree with max_depth = 4 and random_state = 7\n",
                "# TODO Task 1\n",
                "dt = ???\n",
                "\n",
                "#Fit the training data to decision tree\n",
                "# TODO Task 2\n",
                "\n",
                "# Let's see and visualize how our predictions look\n",
                "dt_y_pred = dt.predict(validation_data[features])\n",
                "dt_score = dt.score(validation_data[features], validation_data[target])\n",
                "visualize_confusion_matrix(validation_data[target], dt_y_pred, dt_score)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Discussion:** Now, try to alter the max depth of the tree. What do you see?\n",
                "\n",
                "**Answer:**\n",
                "\n",
                "Now, let's visualize the tree we are making. Note that if you alter the `max_depth` in code below, you will need to save the image to view it in full."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "import graphviz\n",
                "from sklearn import tree\n",
                "\n",
                "\n",
                "def draw_tree(tree_model, features):\n",
                "    \"\"\"\n",
                "    visualizes a Decision Tree\n",
                "    \"\"\"\n",
                "    tree_data = tree.export_graphviz(tree_model, \n",
                "                                    impurity=False, \n",
                "                                    feature_names=features,\n",
                "                                    class_names=tree_model.classes_.astype(str),\n",
                "                                    filled=True,\n",
                "                                    out_file=None)\n",
                "    graph = graphviz.Source(tree_data) \n",
                "    display(graph)\n",
                "    \n",
                "small_tree_model = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
                "small_tree_model.fit(train_data[features], train_data[target])\n",
                "draw_tree(small_tree_model, features)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 6. Implementing a Majority Class Classifier"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we will have you create your own classifier. You will fill in the code below for a Majority Class Classifier. Remember a majority class classifier is simply where every point in the class is assigned to the majority of the training set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "# implement a class\n",
                "class MajorityClassClassifier:\n",
                "    # *NOTE: we can assume we are recieving numpy arrays\n",
                "    #        for our x and y parameters*\n",
                "\n",
                "    def __init__(self):\n",
                "        # input: none\n",
                "        # output: none\n",
                "\n",
                "        # task 3: create a private variable to keep track of\n",
                "        #         the majority label\n",
                "\n",
                "    def fit(self, x, y):\n",
                "        # input: target and label data\n",
                "        # output: none\n",
                "        # Hint: see optional parameters for https://numpy.org/doc/stable/reference/generated/numpy.unique.html\n",
                "\n",
                "        # task 4: find the majority label\n",
                "\n",
                "    def predict(self, x):\n",
                "        # input: target data\n",
                "        # output: predicted results\n",
                "\n",
                "        # task 5: predict using the majority label\n",
                "\n",
                "    def score(self, x, y):\n",
                "        # input: target and label data\n",
                "        # output: accuracy score of our classifier\n",
                "\n",
                "        # task 6: give the accuracy score on the data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 6. Using our implemented class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "# task 7: create your majority class classifier model\n",
                "\n",
                "# task 8: fit the data to the model\n",
                "\n",
                "# task 9: print the testing score from using our implemented class\n",
                "\n",
                "# task 10: predict on x_test and save it to mc_y_pred\n",
                "\n",
                "# task 11: visualize the confusion matrix\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Conceptual Question**: Why is the Majority Class Classifier accuracy so high? \n",
                "\n",
                "**Answer:**\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Below are additional resources for the content covered in this section.\n",
                "\n",
                "* [Dummy Classifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)\n",
                "\n",
                "Further is more Documentation for GridSearch\n",
                "* [What is Grid Search?](https://medium.com/fintechexplained/what-is-grid-search-c01fe886ef0a#:~:text=Grid%20search%20is%20a%20tuning,us%20time%2C%20effort%20and%20resources.)\n",
                "* [Grid Search for model tuning](https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e)\n",
                ""
            ]
        }
    ]
}
