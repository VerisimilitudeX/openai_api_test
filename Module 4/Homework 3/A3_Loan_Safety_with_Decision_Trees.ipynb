{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Loan Safety with Decision Trees\n",
                "\n",
                "The LendingClub is a peer-to-peer lending company that directly connects borrowers and potential lenders/investors. In this notebook, you will build a classification model to predict whether or not a loan provided by LendingClub is likely to default. In this assignment, you will practice:\n",
                "\n",
                "* Use Pandas Dataframes to do feature engineering\n",
                "* Train a decision tree model to predict the sentiment of product reviews.\n",
                "* Visualize the decision tree\n",
                "* Predict the probability of a certain label using the tree\n",
                "* Investigate how the complexity of the tree affects the results\n",
                "\n",
                "Fill in the cells provided marked `TODO` with code to answer the questions. **Unless otherwise noted, every answer you submit should have code that clearly shows the answer in the output.** Answers submitted that do not have associated code that shows the answer may not be accepted for credit. \n",
                "\n",
                "**Make sure to restart the kernel and run all cells** (especially before turning it in) to make sure your code runs correctly. \n",
                "\n",
                "\u003e Copyright ¬©2023 Emily Fox, Hunter Schafer, and Valentina Staneva  All rights reserved.  Permission is hereby granted to students registered for University of Washington CSE/STAT 416 for use solely during Spring Quarter 2024 for purposes of the course.  No other use, copying, distribution, or modification is permitted without prior written consent. Copyrights for third-party components of this work must be honored.  Instructors interested in reusing these course materials should contact the author.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 166,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "\n",
                "%matplotlib inline\n",
                "sns.set()\n",
                "np.random.seed(416)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Investigate Data\n",
                "In this first part of the assignment, we will investigate the data to get a better sense of what we are working with. \n",
                "\n",
                "\u003e Remember that you should look through and understand the parts of the code that load in data since the later parts will assume you are familiar with the preprocessing we did.\n",
                "\n",
                "First we load in the dataset and then inspect the values:\n",
                "\n",
                "**NOTE**: Be sure to run every cell in the notebook! The `###SKIP` is for the autograder. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "\n",
                "# Set seed for the whole program\n",
                "np.random.seed(416)\n",
                "\n",
                "# Load in data\n",
                "loans = pd.read_csv('lending-club-data.csv')\n",
                "loans.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Do not modify the below cell. It configures the autograder, which will award 0 points if it doesn't run.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_load_data) ###"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "One of the features we will use in this assignment is the \"grade\" of the loan. We can investigate what this feature, \"grade\", looks like:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 168,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Want the grades to show up in order from high to low\n",
                "grade_order = sorted(loans['grade'].unique())\n",
                "\n",
                "sns.countplot(x='grade', data=loans, order=grade_order)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can see that over half of the loan grades are assigned values A or B. Each loan is assigned one of these grades, along with a more finely discretized feature called subgrade (feel free to explore that feature column as well!). These values depend on the loan application and credit report, and determine the interest rate of the loan. More information if you're interested can be found [here](https://www.lendingclub.com/investing/investor-education/interest-rates-and-fees).\n",
                "\n",
                "Now let's look at another feature that will be used, \"home ownership\". This feature describes whether the loanee is mortaging, renting, or owns a home. We can see that a small percentage of the loanees own a home."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 169,
            "metadata": {},
            "outputs": [],
            "source": [
                "ownership_order = sorted(loans['home_ownership'].unique())\n",
                "\n",
                "sns.countplot(x='home_ownership', data=loans, order=ownership_order)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Investigate Data - Target\n",
                "The target column (label column) of the dataset that we are interested in is called `bad_loans`. In this column 1 means a risky (bad) loan 0 means a safe loan.\n",
                "\n",
                "In order to make this more intuitive and consistent with the lectures, we reassign the target to be:\n",
                "\n",
                "* +1 as a safe loan,\n",
                "* -1 as a risky (bad) loan.\n",
                "\n",
                "We put this in a new column called `safe_loans`. This column will be the target values you will use while training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 170,
            "metadata": {},
            "outputs": [],
            "source": [
                "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
                "\n",
                "# Drop the old bad_loans column\n",
                "loans = loans.drop(columns='bad_loans')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, let's explore the distribution of values for `safe_loans`. This gives us a sense of how many safe and risky loans are present in the dataset. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 171,
            "metadata": {},
            "outputs": [],
            "source": [
                "only_safe = loans[loans['safe_loans'] == 1]\n",
                "only_bad = loans[loans['safe_loans'] == -1]\n",
                "\n",
                "print(f'Number safe  loans: {len(only_safe)} ({len(only_safe) * 100.0 / len(loans):.2f}%)')\n",
                "print(f'Number risky loans: {len(only_bad)} ({len(only_bad) * 100.0 / len(loans):.2f}%)')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Investigate Data - Questions\n",
                "\n",
                "### **üîç Q1) Most frequent grade**\n",
                "Write code to compute the most frequent value in the `'grade'` column of our dataset.  Store the grade as a `str` in the variable `mode_grade`. You may assume that there is no tie in the most frequent grade.\n",
                "\n",
                "**Like all other problems, unless otherwise specified you need to write code to compute this value in the data rather than hard-coding the answer your find by some other means (e.g., looking at the plot above)**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 172,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q1_most_common_loan_grade) ###\n",
                "\n",
                "# Q1: Write code to find most frequent grade\n",
                "# TODO \n",
                "\n",
                "mode_grade = None"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **üîç Q2) Percentage Rent**\n",
                "What percent of the loans in our dataset are for renting? Write code to compute this number as a value between 0 and 1, and store it in a variable named `percent_rent`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 173,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q2_percent_rent) ###\n",
                "\n",
                "# Q2: Write code to find percent of loans for rent\n",
                "# TODO \n",
                "\n",
                "percent_rent = None"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Preprocess Data for Learning\n",
                "In this section, we pre-process the data to make it suitable for ML.\n",
                "\n",
                "## Encode Categorical Features\n",
                "We will be using both numeric and categorical features to predict if a loan is safe or risky. The features are described in code commments in the next cell. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 174,
            "metadata": {},
            "outputs": [],
            "source": [
                "features = [\n",
                "    'grade',                     # grade of the loan (e.g. A or B)\n",
                "    'sub_grade',                 # sub-grade of the loan (e.g. A1, A2, B1)\n",
                "    'short_emp',                 # one year or less of employment (0 or 1)\n",
                "    'emp_length_num',            # number of years of employment (a number)\n",
                "    'home_ownership',            # home_ownership status (one of own, mortgage, rent or other)\n",
                "    'dti',                       # debt to income ratio (a number)\n",
                "    'purpose',                   # the purpose of the loan (one of many values)\n",
                "    'term',                      # the term of the loan (36 months or 60 months)\n",
                "    'last_delinq_none',          # has borrower had a delinquincy (0 or 1)\n",
                "    'last_major_derog_none',     # has borrower had 90 day or worse rating (0 or 1)\n",
                "    'revol_util',                # percent of available credit being used (number between 0 and 100)\n",
                "    'total_rec_late_fee',        # total late fees received to day (a number)\n",
                "]\n",
                "\n",
                "target = 'safe_loans'                   # prediction target (y) (+1 means safe, -1 is risky)\n",
                "\n",
                "# Extract the feature columns and target column\n",
                "loans = loans[features + [target]]\n",
                "loans.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 175,
            "metadata": {},
            "outputs": [],
            "source": [
                "loans.columns"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "With how `sklearn` implemented its `DecisionTreeClassifier`, it is only able to handle numerical features; the technical reason being it only allows binary splits and assumes (like other `sklearn` models), that the data is only numerical. This means we need to translate the categorical features into numeric ones. \n",
                "\n",
                "The easiest way to do this is a **one-hot encoding** of each categorical feature. A one hot encoding of a feature creates new, derived, features that take on the value 0 or 1 (one new feature for each possible value of the original). This allows us to turn a categorical feature into a series of binary (0 or 1) numerical features.\n",
                "\n",
                "For examples, if we had the dataset shown below where 'Gender' takes on values 'M' (Male), 'F' (Female), 'O' (Other) and 'HasHouse' takes on values 'Y' (Yes), and 'N' (No). This dataset would not work by default in scikit-learn since it has features with categorical values.\n",
                "\n",
                "| Gender | HasHouse | Age |\n",
                "|--------|----------|-----|\n",
                "| M      | N        | 19  |\n",
                "| F      | Y        | 23  |\n",
                "| O      | Y        | 24  |\n",
                "| F      | N        | 21  |\n",
                "\n",
                "To fix this, we could use a one-hot encoding to transform the categorical features into numeric ones. A one-hot encoding of the categorical features would be \n",
                "\n",
                "| Gender_M | Gender_F | Gender_O | HasHouse_N | HasHouse_Y | Age |\n",
                "|----------|----------|----------|------------|------------|-----|\n",
                "| 1        | 0        | 0        | 1          | 0          | 19  |\n",
                "| 0        | 1        | 0        | 0          | 1          | 23  |\n",
                "| 0        | 0        | 1        | 0          | 1          | 24  |\n",
                "| 0        | 1        | 0        | 1          | 0          | 21  |\n",
                "\n",
                "Note that the original 'Gender' column has been transformed into three related columns 'Gender_M', 'Gender_F', and 'Gender_O' which contains zeros or ones depending on the rows' original value. For each row, only one of these derived columns will contain a one (hence the name one-hot encoding).\n",
                "\n",
                "To do this in `pandas`, we use the `get_dummies()` method (another name for one-hot encoding, is dummy variable). We show the new features of the data after running the function in the next cell."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 176,
            "metadata": {},
            "outputs": [],
            "source": [
                "loans = pd.get_dummies(loans)\n",
                "features = list(loans.columns)\n",
                "features.remove('safe_loans')\n",
                "features"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here is a preview of the new dataset. Notice our categorical features have become 0/1s represented over many columns (e.g. \"grade\" becamse \"grade_A\" through \"grade_F\")."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 177,
            "metadata": {},
            "outputs": [],
            "source": [
                "loans.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Model Training\n",
                "In this section, you will start writing code to train the decision tree model. \n",
                "\n",
                "First we split the data into 80% training data and 20% validation data. For this assignment, we will not use a test set since we aren't going to be deploying this model in the future and don't need to make a claim on our accuracy guarantee. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 178,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "train_data, validation_data = train_test_split(loans, test_size=0.2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **üîç Q3) Train first classifier**\n",
                "\n",
                "Now let's use `sklearn`'s `DecisionTreeClassifier` (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)) \n",
                "to create a loan-safety prediction model on the training data. \n",
                "\n",
                "Train a decision tree classifier for this data. When creating the model you should only pass in the values `max_depth=6`. Use the features and target defined earlier when training the model on the training data. Save the tree in a variable called `decision_tree_model`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 202,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q3_decision_tree_model) ###\n",
                "\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "# Q3: Train a model with max_depth=6\n",
                "# TODO \n",
                "decision_tree_model = None\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we want you to visualize what the tree looks like. In the cell below, we have defined a function to visualize the tree. \n",
                "\n",
                "Visualizing a depth 7 tree can be hard, so instead we will train another model stored in a variable called `small_tree_model` that has `max_depth=2`.\n",
                "\n",
                "Installing Graphviz (especially on Windows) can be kind of tricky. If the code below doesn't run correctly, we provide an image to the picture you should have seen! "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 180,
            "metadata": {},
            "outputs": [],
            "source": [
                "import graphviz\n",
                "from sklearn import tree\n",
                "\n",
                "\n",
                "def draw_tree(tree_model, features):\n",
                "    \"\"\"\n",
                "    visualizes a Decision Tree\n",
                "    \"\"\"\n",
                "    tree_data = tree.export_graphviz(tree_model, \n",
                "                                    impurity=False, \n",
                "                                    feature_names=features,\n",
                "                                    class_names=tree_model.classes_.astype(str),\n",
                "                                    filled=True,\n",
                "                                    out_file=None)\n",
                "    graph = graphviz.Source(tree_data) \n",
                "    display(graph)\n",
                "    \n",
                "small_tree_model = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
                "small_tree_model.fit(train_data[features], train_data[target])\n",
                "draw_tree(small_tree_model, features)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Each node in the tree shows\n",
                "* If it's an internal node, show what feature and value it splits on\n",
                "* `samples`: The number of samples at that node\n",
                "* `value`: The counts for each label at that node\n",
                "* `class`: The majority class at that node\n",
                "* The color showing how confident it is in the predictions at that node (blue is class +1, orange is class -1, white is in between). This corresponds to the probability of it predicting a certain label.\n",
                "\n",
                "## Making Predictions\n",
                "Now that we have trained the model, let's look at how it makes predictions on our data.\n",
                "\n",
                "### **üîç Q4) Accuracies**\n",
                "Compute the training accuracy and validation accuracy using both your `decision_tree_model`. Calculate which percentage of those examples it classified correctly as a number between 0 and 1. So for this problem, you will need to make two variables:\n",
                "* `decision_train_accuracy` for `decision_tree_model`'s training accuracy\n",
                "* `decision_validation_accuracy` for `decision_tree_model`'s validation accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 181,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q4_validation_accuracy) ###\n",
                "\n",
                "# Q4: Find train and validation accuracy\n",
                "# TODO \n",
                "\n",
                "decision_train_accuracy = None\n",
                "decision_validation_accuracy = None"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **üîç Q5) Tall Tree**\n",
                "Next, you should train another decision tree model with `max_depth=10`. This will make a much deeper tree. \n",
                "\n",
                "In the cell below, train the model on the training data and report its training and validation accuracy. Save the model in a variable called `big_tree_model`, the training accuracy in a variable called `big_train_accuracy` and validation accuracy `big_validation_accuracy`.\n",
                "\n",
                "Look back at the values in the last question to see if your values here make sense with what you know about heights of trees."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 182,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q5_big_tree) ###\n",
                "\n",
                "# Q5: Train a decision tree model with max_depth=10\n",
                "# TODO \n",
                "\n",
                "big_tree_model = None\n",
                "big_train_accuracy = None\n",
                "big_validation_accuracy = None\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# **Q6) üîç Finding Right Fit**\n",
                "As we saw in the previous example, if we aren't careful with how we set `max_depth`, our model can potentially overfit or underfit. \n",
                "\n",
                "A common way of limiting the depth of the tree is to consider two early stopping rules:\n",
                "* When when the tree reaches a maximum depth (`max_depth`)\n",
                "* When the leaf nodes have too few data samples in them (`min_samples_leaf`)\n",
                "\n",
                "Instead of writing the standard hyper-parameter tuning loop you've written before (i.e. loop over possible choices of something like $\\lambda$, train on the training data with that value of $\\lambda$, and validate using a validation set), we will use a `sklearn` module that does this for us using k-fold cross validation.\n",
                "\n",
                "The reason we want to use a library for this task is it gets more complicated when we want to find the best combination of both `max_depth` and `min_samples_leaf`. The code becomes quite tedious to write since your loop would need to try all pairs of values for `max_depth` and `min_samples_leaf` and have some way of storing the results to compare.\n",
                "\n",
                "Instead, we use use `sklearn`'s `GridSearchCV` (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)) to do all this book-keeping for us! You should look at the documentation on that page (there is a good example underneath the method reference) to solve the following step.\n",
                "\n",
                "Use `GridSearchCV` to try all combinations of \n",
                "* `min_samples_leaf`: [1, 10, 50, 100, 200, 300]\n",
                "* `max_depth`: [1, 5, 10, 15, 20]\n",
                "\n",
                "Some implementation details:\n",
                "* For our code later, save the `GridSearchCV` object in a variable called `search` and the dictionary specifying parameters in a variable called `hyperparameters`.\n",
                "* You should use 6-fold valudation `cv=6` and make sure it records the training accuracies by using `return_train_score=True`.\n",
                "* This might take some time to run! "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 183,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q6_grid_search) ###\n",
                "\n",
                "# Q6: Use GridSearchCV to find best settings of hyperparameters\n",
                "# TODO \n",
                "\n",
                "search = None"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Print the result using the `best_params_` property on the `GridSearchCV` object. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(search.best_params_)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also plot the train and validation accuracy of the models for different settings of the hyper-parameters. The plot will be in 3D since there are 2 inputs for each model specification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 184,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_scores(ax, title, search, hyperparameters, score_key):\n",
                "    # Get results from GridSearch and turn scores into matrix\n",
                "    cv_results = search.cv_results_\n",
                "    scores = cv_results[score_key]\n",
                "    scores = scores.reshape((len(hyperparameters['max_depth']), len(hyperparameters['min_samples_leaf'])))\n",
                "    max_depths = cv_results['param_max_depth'].reshape(scores.shape).data.astype(int)\n",
                "    min_samples_leafs = cv_results['param_min_samples_leaf'].reshape(scores.shape).data.astype(int)\n",
                "    \n",
                "    # Plot result\n",
                "    ax.plot_wireframe(max_depths, min_samples_leafs, scores)\n",
                "    ax.view_init(20, 220)\n",
                "    ax.set_xlabel('Maximum Depth')\n",
                "    ax.set_ylabel('Minimum Samples Leaf')\n",
                "    ax.set_zlabel('Accuracy')\n",
                "    ax.set_title(title)\n",
                "\n",
                "\n",
                "fig = plt.figure(figsize=(15,7))\n",
                "ax1 = fig.add_subplot(121, projection='3d')\n",
                "ax2 = fig.add_subplot(122, projection='3d')\n",
                "plot_scores(ax1, 'Train Accuracy', search, hyperparameters, 'mean_train_score')\n",
                "plot_scores(ax2, 'Validation Accuracy', search, hyperparameters, 'mean_test_score')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There is no question for this part, but make sure you understand why we see the trends we do here. Make sure you can answer the following questions:\n",
                "* Why does the train accuracy spike up in the left graph, but go downward on the right graph?\n",
                "* Which graph should we look at to pick the model that will hopefully do best in the future?\n",
                "* Why is the spike in training accuracy only for the point with `max_depth=20` and `min_samples_leaf=1`? Why isn't the training accuracy very high for the other settings of `min_samples_leaf` even when `max_depth=20`?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                "# **üîç Q7) Random Forest**\n",
                "In class, we talked about the random forest ensemble. To show us empirically that such a simple idea of adding multiple classifiers works in practice, in this section we are going to implement a type of random forest and analyze its performance. \n",
                "\n",
                "This is slightly different than most work you'll do in our course since we are asking you to implement the inner-workings of part of a model. You should write code to make the ensemble, but you can use `sklearn`'s decision tree model as the models in your ensemble; in other words you don't need to implement a decision tree from scratch! \n",
                "\n",
                "Below, we have written the starter code for a class called `RandomForest416` that has methods similar to most sklearn models. You only have to implement the `fit` method, all other parts are implemented for you. However, you should understand how the other code works to help you identify what you need to do. Besides, this implement doesn't use parallelization like in practice, but technically each iteration is independent of the others.\n",
                "\n",
                "The `fit` method should train each tree on a different random sample of the examples in the given dataset with $n$ examples. Each random sample should select examples uniformly at random (with replacement) to make a new, modified, dataset of $n$ examples for that tree. To do this, you will want to use `np.random.randint` (documentation [here](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html)) to generate a random sequence of indices, and then use the `iloc` property on the given `pandas` objects to select those rows. You do **not** need to randomly select the subset of features to use for each sample of the dataset; you can simply use all the features for each tree."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 185,
            "metadata": {},
            "outputs": [],
            "source": [
                "import scipy.stats \n",
                "\n",
                "class RandomForest416: \n",
                "    \"\"\"\n",
                "    This class implements the common sklearn model interface (has a fit and predict function).\n",
                "    \n",
                "    A random forest is a collection of decision trees that are trained on random subsets of the \n",
                "    dataset. When predicting the value for an example, takes a majority vote from the trees.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, num_trees, max_depth=1):\n",
                "        \"\"\"\n",
                "        Constructs a RandomForest416 that uses the given numbner of trees, each with a \n",
                "        max depth of max_depth.\n",
                "        \"\"\"\n",
                "        self._trees = [\n",
                "            DecisionTreeClassifier(max_depth=max_depth) \n",
                "            for i in range(num_trees)\n",
                "        ]\n",
                "        \n",
                "    def fit(self, X, y):\n",
                "        \"\"\"\n",
                "        Takes an input dataset X and a series of targets y and trains the RandomForest416.\n",
                "        \n",
                "        Each tree will be trained on a random sample of the data that samples the examples\n",
                "        uniformly at random (with replacement). Each random dataset will have the same number\n",
                "        of examples as the original dataset, but some examples may be missing or appear more \n",
                "        than once due to the random sampling with replacement.\n",
                "        \"\"\"    \n",
                "        # Q7\n",
                "        # TODO \n",
                "           \n",
                "    def predict(self, X):\n",
                "        \"\"\"\n",
                "        Takes an input dataset X and returns the predictions for each example in X.\n",
                "        \"\"\"\n",
                "        # Builds up a 2d array with n rows and T columns\n",
                "        # where n is the number of points to classify and T is the number of trees\n",
                "        predictions = np.zeros((len(X), len(self._trees)))\n",
                "        for i, tree in enumerate(self._trees):\n",
                "            # Make predictions using the current tree\n",
                "            preds = tree.predict(X)\n",
                "            \n",
                "            # Store those predictions in ith column of the 2d array\n",
                "            predictions[:, i] = preds\n",
                "            \n",
                "        # For each row of predictions, find the most frequent label (axis=1 means across columns)\n",
                "return scipy.stats.mode(predictions, axis=1, keepdims=False).mode "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can test out your implementation by creating RandomForest416 object using 2 decision trees, each with a max depth of 1. Save the training accuracy and validation accuracy of this model with `rf_train_accuracy` and `rf_validation_accuracy`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 186,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q7_random_forest) ###\n",
                "\n",
                "# Q7\n",
                "# TODO\n",
                "\n",
                "rf_train_accuracy = None\n",
                "rf_validation_accuracy = None"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can then compare how `sklearn`'s `DecisionTreeClassifier` to our `RandomForest416` to see how its training and validation accuracies compare as a function of the depth of the trees. The code below trains each model with a different max depth and then plots their accuracies. It might take some time to run, but it also prints the progress by showing the current depth it is on (it will range from 1 to 25)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 187,
            "metadata": {},
            "outputs": [],
            "source": [
                "# First calculate the accuracies for each depth\n",
                "depths = list(range(1, 26, 2))\n",
                "dt_accuracies = []\n",
                "rf_accuracies = []\n",
                "\n",
                "for i in depths:\n",
                "    print(f'Depth {i}')\n",
                "\n",
                "    # Train and evaluate a Decision Tree Classifier with given max_depth\n",
                "    tree = DecisionTreeClassifier(max_depth=i)\n",
                "    tree.fit(train_data[features], train_data[target])\n",
                "    dt_accuracies.append((\n",
                "        accuracy_score(tree.predict(train_data[features]), train_data[target]),\n",
                "        accuracy_score(tree.predict(validation_data[features]), validation_data[target])\n",
                "    ))\n",
                "    \n",
                "    # Train and evaluate our RandomForest classifier with given max_depth \n",
                "    rf = RandomForest416(15, max_depth=i)\n",
                "    rf.fit(train_data[features], train_data[target])\n",
                "    rf_accuracies.append((     \n",
                "        accuracy_score(rf.predict(train_data[features]), train_data[target]),\n",
                "        accuracy_score(rf.predict(validation_data[features]), validation_data[target])\n",
                "    ))\n",
                "    \n",
                "# Then plot the scores\n",
                "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Plot training accuracies\n",
                "axs[0].plot(depths, [acc[0] for acc in dt_accuracies], label='DecisionTree')\n",
                "axs[0].plot(depths, [acc[0] for acc in rf_accuracies], label='RandomForest416')\n",
                "\n",
                "# Plot validation accuracies\n",
                "axs[1].plot(depths, [acc[1] for acc in dt_accuracies], label='DecisionTree')\n",
                "axs[1].plot(depths, [acc[1] for acc in rf_accuracies], label='RandomForest416')\n",
                "\n",
                "# Customize plots\n",
                "axs[0].set_title('Train Data')\n",
                "axs[1].set_title('Validation Data')\n",
                "for ax in axs:\n",
                "    ax.legend()\n",
                "    ax.set_xlabel('Max Depth')\n",
                "    ax.set_ylabel('Accuracy')"
            ]
        }
    ]
}
